{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1182f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Initial memory usage: 0.81 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import warnings\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import psutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024 / 1024  # GB\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Initial memory usage: {get_memory_usage():.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2674e8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MEMORY-EFFICIENT DATA LOADING ===\n",
      "03-11/LDAP.csv: 2,113,234 rows\n",
      "03-11/MSSQL.csv: 5,775,786 rows\n",
      "03-11/NetBIOS.csv: 3,455,899 rows\n",
      "03-11/Portmap.csv: 191,694 rows\n",
      "03-11/Syn.csv: 4,320,541 rows\n",
      "03-11/UDP.csv: 3,782,206 rows\n",
      "03-11/UDPLag.csv: 725,165 rows\n",
      "Total rows across all files: 20,364,525\n",
      "Will sample 2,036,452 rows (10.0%)\n",
      "\n",
      "Processing 03-11/LDAP.csv...\n",
      "  Processed 10 chunks, 10,000 rows, Memory: 0.82 GB\n",
      "  Processed 20 chunks, 20,000 rows, Memory: 0.83 GB\n",
      "  Processed 30 chunks, 30,000 rows, Memory: 0.83 GB\n",
      "  Processed 40 chunks, 40,000 rows, Memory: 0.84 GB\n",
      "  Processed 50 chunks, 50,000 rows, Memory: 0.84 GB\n",
      "  Processed 60 chunks, 60,000 rows, Memory: 0.85 GB\n",
      "  Processed 70 chunks, 70,000 rows, Memory: 0.86 GB\n",
      "  Processed 80 chunks, 80,000 rows, Memory: 0.86 GB\n",
      "  Processed 90 chunks, 90,000 rows, Memory: 0.87 GB\n",
      "  Processed 100 chunks, 100,000 rows, Memory: 0.87 GB\n",
      "  Processed 110 chunks, 110,000 rows, Memory: 0.88 GB\n",
      "  Processed 120 chunks, 120,000 rows, Memory: 0.89 GB\n",
      "  Processed 130 chunks, 130,000 rows, Memory: 0.89 GB\n",
      "  Processed 140 chunks, 140,000 rows, Memory: 0.90 GB\n",
      "  Processed 150 chunks, 150,000 rows, Memory: 0.90 GB\n",
      "  Processed 160 chunks, 160,000 rows, Memory: 0.91 GB\n",
      "  Processed 170 chunks, 170,000 rows, Memory: 0.91 GB\n",
      "  Processed 180 chunks, 180,000 rows, Memory: 0.92 GB\n",
      "  Processed 190 chunks, 190,000 rows, Memory: 0.93 GB\n",
      "  Processed 200 chunks, 200,000 rows, Memory: 0.93 GB\n",
      "  Processed 210 chunks, 210,000 rows, Memory: 0.94 GB\n",
      "\n",
      "Processing 03-11/MSSQL.csv...\n",
      "  Processed 10 chunks, 221,323 rows, Memory: 0.95 GB\n",
      "  Processed 20 chunks, 231,323 rows, Memory: 0.95 GB\n",
      "  Processed 30 chunks, 241,323 rows, Memory: 0.96 GB\n",
      "  Processed 40 chunks, 251,323 rows, Memory: 0.96 GB\n",
      "  Processed 50 chunks, 261,323 rows, Memory: 0.97 GB\n",
      "  Processed 60 chunks, 271,323 rows, Memory: 0.98 GB\n",
      "  Processed 70 chunks, 281,323 rows, Memory: 0.98 GB\n",
      "  Processed 80 chunks, 291,323 rows, Memory: 0.99 GB\n",
      "  Processed 90 chunks, 301,323 rows, Memory: 0.99 GB\n",
      "  Processed 100 chunks, 311,323 rows, Memory: 1.00 GB\n",
      "  Processed 110 chunks, 321,323 rows, Memory: 1.01 GB\n",
      "  Processed 120 chunks, 331,323 rows, Memory: 1.01 GB\n",
      "  Processed 130 chunks, 341,323 rows, Memory: 1.02 GB\n",
      "  Processed 140 chunks, 351,323 rows, Memory: 1.03 GB\n",
      "  Processed 150 chunks, 361,323 rows, Memory: 1.03 GB\n",
      "  Processed 160 chunks, 371,323 rows, Memory: 1.04 GB\n",
      "  Processed 170 chunks, 381,323 rows, Memory: 1.05 GB\n",
      "  Processed 180 chunks, 391,323 rows, Memory: 1.05 GB\n",
      "  Processed 190 chunks, 401,323 rows, Memory: 1.06 GB\n",
      "  Processed 200 chunks, 411,323 rows, Memory: 1.07 GB\n",
      "  Processed 210 chunks, 421,323 rows, Memory: 1.07 GB\n",
      "  Processed 220 chunks, 431,323 rows, Memory: 1.08 GB\n",
      "  Processed 230 chunks, 441,323 rows, Memory: 1.08 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m SAMPLE_FRACTION \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Adjust this: 0.1 = 10%, 0.05 = 5%, None = full dataset\u001b[39;00m\n\u001b[0;32m    103\u001b[0m CHUNK_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m     \u001b[38;5;66;03m# Reduce if still getting memory errors\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m df, file_info \u001b[38;5;241m=\u001b[39m \u001b[43mload_csvs_efficiently\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43msample_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLE_FRACTION\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m, in \u001b[0;36mload_csvs_efficiently\u001b[1;34m(csv_files, chunk_size, sample_fraction)\u001b[0m\n\u001b[0;32m     55\u001b[0m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_file\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Optimize dtypes to save memory\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m all_chunks\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[0;32m     61\u001b[0m rows_processed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "Cell \u001b[1;32mIn[4], line 93\u001b[0m, in \u001b[0;36moptimize_dtypes\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     90\u001b[0m             df[col] \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m df[col]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m         df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\numeric.py:281\u001b[0m, in \u001b[0;36mto_numeric\u001b[1;34m(arg, errors, downcast, dtype_backend)\u001b[0m\n\u001b[0;32m    279\u001b[0m dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(typecode)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize:\n\u001b[1;32m--> 281\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_downcast_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# successful conversion\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype:\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:404\u001b[0m, in \u001b[0;36mmaybe_downcast_numeric\u001b[1;34m(result, dtype, do_round)\u001b[0m\n\u001b[0;32m    400\u001b[0m     atol \u001b[38;5;241m=\u001b[39m size_tols\u001b[38;5;241m.\u001b[39mget(new_result\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Check downcast float values are still equal within 7 digits when\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# converting from float64 to float32\u001b[39;00m\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m new_result\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\numeric.py:2241\u001b[0m, in \u001b[0;36mallclose\u001b[1;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_allclose_dispatcher)\n\u001b[0;32m   2171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallclose\u001b[39m(a, b, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.e-5\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.e-8\u001b[39m, equal_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   2172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2173\u001b[0m \u001b[38;5;124;03m    Returns True if two arrays are element-wise equal within a tolerance.\u001b[39;00m\n\u001b[0;32m   2174\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2239\u001b[0m \n\u001b[0;32m   2240\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2241\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(res)\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\numeric.py:2351\u001b[0m, in \u001b[0;36misclose\u001b[1;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[0;32m   2349\u001b[0m yfin \u001b[38;5;241m=\u001b[39m isfinite(y)\n\u001b[0;32m   2350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(xfin) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(yfin):\n\u001b[1;32m-> 2351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwithin_tol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2353\u001b[0m     finite \u001b[38;5;241m=\u001b[39m xfin \u001b[38;5;241m&\u001b[39m yfin\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\numeric.py:2332\u001b[0m, in \u001b[0;36misclose.<locals>.within_tol\u001b[1;34m(x, y, atol, rtol)\u001b[0m\n\u001b[0;32m   2330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwithin_tol\u001b[39m(x, y, atol, rtol):\n\u001b[0;32m   2331\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m errstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m), _no_nep50_warning():\n\u001b[1;32m-> 2332\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m less_equal(\u001b[38;5;28mabs\u001b[39m(x\u001b[38;5;241m-\u001b[39my), atol \u001b[38;5;241m+\u001b[39m rtol \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mabs\u001b[39m(y))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_csvs_efficiently(csv_files, chunk_size=50000, sample_fraction=None):\n",
    "    \"\"\"\n",
    "    Load CSV files efficiently using chunking to avoid memory overload\n",
    "    \"\"\"\n",
    "    print(\"=== MEMORY-EFFICIENT DATA LOADING ===\")\n",
    "    \n",
    "    file_info = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for file_pattern in csv_files:\n",
    "        try:\n",
    "            files = glob.glob(file_pattern)\n",
    "            if not files:\n",
    "                files = glob.glob(f\"*{file_pattern}*\")\n",
    "            \n",
    "            for file in files:\n",
    "                with open(file, 'r') as f:\n",
    "                    row_count = sum(1 for line in f) - 1  # subtract header\n",
    "                \n",
    "                file_info.append((file, row_count))\n",
    "                total_rows += row_count\n",
    "                print(f\"{file}: {row_count:,} rows\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking {file_pattern}: {e}\")\n",
    "    \n",
    "    print(f\"Total rows across all files: {total_rows:,}\")\n",
    "    \n",
    "    # If sampling is requested, calculate sample size\n",
    "    if sample_fraction:\n",
    "        sample_size = int(total_rows * sample_fraction)\n",
    "        print(f\"Will sample {sample_size:,} rows ({sample_fraction*100:.1f}%)\")\n",
    "    \n",
    "    # Load data in chunks\n",
    "    all_chunks = []\n",
    "    rows_processed = 0\n",
    "    \n",
    "    for file, row_count in file_info:\n",
    "        print(f\"\\nProcessing {file}...\")\n",
    "        \n",
    "        # Calculate skip probability for sampling\n",
    "        if sample_fraction:\n",
    "            skip_prob = 1 - sample_fraction\n",
    "        else:\n",
    "            skip_prob = 0\n",
    "        \n",
    "        chunk_iter = pd.read_csv(file, chunksize=chunk_size, low_memory=True)\n",
    "        \n",
    "        for i, chunk in enumerate(chunk_iter):\n",
    "            # Sample chunk if needed\n",
    "            if sample_fraction and sample_fraction < 1.0:\n",
    "                chunk = chunk.sample(frac=sample_fraction, random_state=42)\n",
    "            \n",
    "            # Add source file info\n",
    "            chunk['source_file'] = file.split('/')[-1].replace('.csv', '')\n",
    "            \n",
    "            # Optimize dtypes to save memory\n",
    "            chunk = optimize_dtypes(chunk)\n",
    "            \n",
    "            all_chunks.append(chunk)\n",
    "            rows_processed += len(chunk)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i+1} chunks, {rows_processed:,} rows, Memory: {get_memory_usage():.2f} GB\")\n",
    "                \n",
    "                # Force garbage collection\n",
    "                gc.collect()\n",
    "    \n",
    "    print(f\"\\nCombining {len(all_chunks)} chunks...\")\n",
    "    combined_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    \n",
    "    # Clear chunks from memory\n",
    "    del all_chunks\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Final dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Memory usage after loading: {get_memory_usage():.2f} GB\")\n",
    "    \n",
    "    return combined_df, file_info\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"Optimize data types to reduce memory usage\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            if df[col].min() >= -128 and df[col].max() <= 127:\n",
    "                df[col] = df[col].astype('int8')\n",
    "            elif df[col].min() >= -32768 and df[col].max() <= 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            elif df[col].min() >= -2147483648 and df[col].max() <= 2147483647:\n",
    "                df[col] = df[col].astype('int32')\n",
    "        \n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    return df\n",
    "\n",
    "csv_files = ['03-11/LDAP.csv', '03-11/MSSQL.csv', '03-11/NetBIOS.csv', '03-11/Portmap.csv', '03-11/Syn.csv', '03-11/UDP.csv', '03-11/UDPLag.csv']\n",
    "\n",
    "SAMPLE_FRACTION = 0.1  # Adjust this: 0.1 = 10%, 0.05 = 5%, None = full dataset\n",
    "CHUNK_SIZE = 10000     # Reduce if still getting memory errors\n",
    "\n",
    "df, file_info = load_csvs_efficiently(csv_files, \n",
    "                                     chunk_size=CHUNK_SIZE, \n",
    "                                     sample_fraction=SAMPLE_FRACTION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA EXPLORATION ===\n",
      "Dataset shape: (2036452, 89)\n",
      "Memory usage: 1.61 GB\n",
      "Current system memory: 1.59 GB\n",
      "\n",
      "Column data types (from sample):\n",
      "float64    25\n",
      "int8       20\n",
      "float32    20\n",
      "int32      11\n",
      "object      7\n",
      "int16       3\n",
      "int64       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few rows:\n",
      "   Unnamed: 0                               Flow ID   Source IP   Source Port  \\\n",
      "0       18347  172.16.0.5-192.168.50.4-580-39068-17  172.16.0.5           580   \n",
      "1      185707   172.16.0.5-192.168.50.4-790-2619-17  172.16.0.5           790   \n",
      "2       61775  172.16.0.5-192.168.50.4-821-57411-17  172.16.0.5           821   \n",
      "3      198824  172.16.0.5-192.168.50.4-841-21511-17  172.16.0.5           841   \n",
      "4        6466  172.16.0.5-192.168.50.4-649-56838-17  172.16.0.5           649   \n",
      "\n",
      "   Destination IP   Destination Port   Protocol                   Timestamp  \\\n",
      "0    192.168.50.4              39068         17  2018-11-03 10:09:01.351731   \n",
      "1    192.168.50.4               2619         17  2018-11-03 10:09:01.158536   \n",
      "2    192.168.50.4              57411         17  2018-11-03 10:09:00.796315   \n",
      "3    192.168.50.4              21511         17  2018-11-03 10:09:01.165035   \n",
      "4    192.168.50.4              56838         17  2018-11-03 10:09:01.142124   \n",
      "\n",
      "    Flow Duration   Total Fwd Packets  ...   Active Max   Active Min  \\\n",
      "0               1                   2  ...          0.0          0.0   \n",
      "1               1                   2  ...          0.0          0.0   \n",
      "2               1                   2  ...          0.0          0.0   \n",
      "3              47                   2  ...          0.0          0.0   \n",
      "4               1                   2  ...          0.0          0.0   \n",
      "\n",
      "   Idle Mean   Idle Std   Idle Max   Idle Min  SimillarHTTP   Inbound  \\\n",
      "0        0.0        0.0        0.0        0.0             0         1   \n",
      "1        0.0        0.0        0.0        0.0             0         1   \n",
      "2        0.0        0.0        0.0        0.0             0         1   \n",
      "3        0.0        0.0        0.0        0.0             0         1   \n",
      "4        0.0        0.0        0.0        0.0             0         1   \n",
      "\n",
      "     Label  source_file  \n",
      "0  NetBIOS         LDAP  \n",
      "1  NetBIOS         LDAP  \n",
      "2  NetBIOS         LDAP  \n",
      "3  NetBIOS         LDAP  \n",
      "4  NetBIOS         LDAP  \n",
      "\n",
      "[5 rows x 89 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Memory-Efficient Data Exploration\n",
    "print(\"\\n=== DATA EXPLORATION ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "print(f\"Current system memory: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "# Sample exploration to avoid memory issues\n",
    "sample_df = df.sample(n=min(1000, len(df)), random_state=42)\n",
    "print(f\"\\nColumn data types (from sample):\")\n",
    "print(sample_df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Memory cleanup\n",
    "del sample_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MEMORY-EFFICIENT MISSING VALUE HANDLING ===\n",
      "Checking missing values...\n",
      "Columns with missing values: 1\n",
      "Flow Bytes/s: Will fill with median 458000000.0\n",
      "Missing values handled!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Memory-Efficient Missing Value Handling\n",
    "print(\"\\n=== MEMORY-EFFICIENT MISSING VALUE HANDLING ===\")\n",
    "\n",
    "def handle_missing_values_efficiently(df, batch_size=100000):\n",
    "    \"\"\"Handle missing values in batches to avoid memory issues\"\"\"\n",
    "    \n",
    "    print(\"Checking missing values...\")\n",
    "    missing_info = df.isnull().sum()\n",
    "    cols_with_missing = missing_info[missing_info > 0].index.tolist()\n",
    "    \n",
    "    if len(cols_with_missing) == 0:\n",
    "        print(\"No missing values found!\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Columns with missing values: {len(cols_with_missing)}\")\n",
    "    \n",
    "    # Calculate fill values first\n",
    "    fill_values = {}\n",
    "    \n",
    "    for col in cols_with_missing:\n",
    "        if df[col].dtype in ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n",
    "            fill_values[col] = df[col].median()\n",
    "            print(f\"{col}: Will fill with median {fill_values[col]}\")\n",
    "        else:\n",
    "            mode_val = df[col].mode()\n",
    "            fill_values[col] = mode_val[0] if len(mode_val) > 0 else 'unknown'\n",
    "            print(f\"{col}: Will fill with mode '{fill_values[col]}'\")\n",
    "    \n",
    "    # Fill missing values\n",
    "    df.fillna(fill_values, inplace=True)\n",
    "    \n",
    "    print(\"Missing values handled!\")\n",
    "    return df\n",
    "\n",
    "df = handle_missing_values_efficiently(df)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13843fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MEMORY-EFFICIENT DATA PREPARATION ===\n",
      "Potential target:  Protocol (3 unique values)\n",
      "Using target column:  Protocol\n",
      "Target distribution:\n",
      " Protocol\n",
      "17    1543541\n",
      "6      492767\n",
      "0         144\n",
      "Name: count, dtype: int64\n",
      "Feature matrix shape: (2036452, 87)\n",
      "Memory usage: 1.49 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Memory-Efficient Data Preparation\n",
    "print(\"\\n=== MEMORY-EFFICIENT DATA PREPARATION ===\")\n",
    "\n",
    "# Find target column\n",
    "possible_targets = ['label', 'target', 'attack_type', 'class', 'Label', 'Target', 'Attack_Type', 'Class']\n",
    "target_col = None\n",
    "\n",
    "for col in possible_targets:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    # Look for categorical columns with few unique values\n",
    "    for col in df.columns:\n",
    "        if col != 'source_file' and df[col].nunique() < 50:\n",
    "            print(f\"Potential target: {col} ({df[col].nunique()} unique values)\")\n",
    "            target_col = col\n",
    "            break\n",
    "\n",
    "if target_col is None:\n",
    "    target_col = df.columns[-2]  # Assume second to last (before source_file)\n",
    "\n",
    "print(f\"Using target column: {target_col}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop([target_col, 'source_file'], axis=1, errors='ignore')\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# Clean up original dataframe\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Memory usage: {get_memory_usage():.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a5543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EFFICIENT ENCODING AND SCALING ===\n",
      "Encoding 6 categorical columns...\n",
      "Encoded: Flow ID\n",
      "Encoded:  Source IP\n",
      "Encoded:  Destination IP\n",
      "Encoded:  Timestamp\n",
      "Encoded: SimillarHTTP\n",
      "Encoded:  Label\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6: Efficient Encoding and Scaling\n",
    "print(\"\\n=== EFFICIENT ENCODING AND SCALING ===\")\n",
    "\n",
    "# Encode categorical features efficiently\n",
    "label_encoders = {}\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Encoding {len(categorical_cols)} categorical columns...\")\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded: {col}\")\n",
    "\n",
    "# Encode target if categorical\n",
    "target_encoder = None\n",
    "if y.dtype == 'object':\n",
    "    target_encoder = LabelEncoder()\n",
    "    y = target_encoder.fit_transform(y)\n",
    "    print(\"Target encoded\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b57968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MEMORY-EFFICIENT DATA SPLITTING ===\n",
      "Test size: 0.0982 (200,000 samples)\n",
      "Training set: (1836452, 87)\n",
      "Test set: (200000, 87)\n",
      "Memory usage: 2.26 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7: Memory-Efficient Train-Test Split\n",
    "print(\"\\n=== MEMORY-EFFICIENT DATA SPLITTING ===\")\n",
    "\n",
    "# For very large datasets, use stratified sampling for test set\n",
    "test_size = min(0.2, 200000 / len(X))  # Cap test set at 200k samples\n",
    "print(f\"Test size: {test_size:.4f} ({int(len(X) * test_size):,} samples)\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Memory usage: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "# Clean up full datasets\n",
    "del X, y\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f1575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BATCH SCALING ===\n",
      "Fitting scaler in batches...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scaler\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Fit scaler\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mfit_scaler_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_in_batches\u001b[39m(scaler, X, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m):\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform data in batches\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m, in \u001b[0;36mfit_scaler_in_batches\u001b[1;34m(X_train, batch_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_idx \u001b[38;5;241m+\u001b[39m batch_size, n_samples)\n\u001b[0;32m     13\u001b[0m batch \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39miloc[start_idx:end_idx]\n\u001b[1;32m---> 15\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (start_idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Processed batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(n_samples\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:914\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    913\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 914\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\srujan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Cell 8: Batch Scaling to Avoid Memory Issues\n",
    "print(\"\\n=== BATCH SCALING ===\")\n",
    "\n",
    "def fit_scaler_in_batches(X_train, batch_size=50000):\n",
    "    \"\"\"Fit scaler using batches to avoid memory issues\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    print(\"Fitting scaler in batches...\")\n",
    "    n_samples = len(X_train)\n",
    "    \n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch = X_train.iloc[start_idx:end_idx]\n",
    "        \n",
    "        scaler.partial_fit(batch)\n",
    "        \n",
    "        if (start_idx // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Processed batch {start_idx // batch_size + 1}/{(n_samples - 1) // batch_size + 1}\")\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "# Fit scaler\n",
    "scaler = fit_scaler_in_batches(X_train, batch_size=50000)\n",
    "\n",
    "def transform_in_batches(scaler, X, batch_size=50000):\n",
    "    \"\"\"Transform data in batches\"\"\"\n",
    "    n_samples = len(X)\n",
    "    transformed_batches = []\n",
    "    \n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch = X.iloc[start_idx:end_idx]\n",
    "        transformed_batch = scaler.transform(batch)\n",
    "        transformed_batches.append(transformed_batch)\n",
    "    \n",
    "    return np.vstack(transformed_batches)\n",
    "\n",
    "print(\"Scaling training data...\")\n",
    "X_train_scaled = transform_in_batches(scaler, X_train)\n",
    "print(\"Scaling test data...\")\n",
    "X_test_scaled = transform_in_batches(scaler, X_test)\n",
    "\n",
    "print(f\"Scaling completed. Memory: {get_memory_usage():.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a897ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Memory-Efficient Model Training\n",
    "print(\"\\n=== MEMORY-EFFICIENT MODEL TRAINING ===\")\n",
    "\n",
    "# Use smaller Random Forest for large datasets\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=50,      # Reduced from 100\n",
    "    max_depth=15,         # Reduced from 20\n",
    "    min_samples_split=10, # Increased\n",
    "    min_samples_leaf=5,   # Increased\n",
    "    max_features='sqrt',  # Use sqrt instead of all features\n",
    "    random_state=42,\n",
    "    n_jobs=2,            # Limit parallel jobs to save memory\n",
    "    verbose=1            # Show progress\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "print(f\"Training samples: {X_train_scaled.shape[0]:,}\")\n",
    "print(f\"Features: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774aff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Batch Prediction and Evaluation\n",
    "print(\"\\n=== BATCH PREDICTION AND EVALUATION ===\")\n",
    "\n",
    "def predict_in_batches(model, X, batch_size=10000):\n",
    "    \"\"\"Make predictions in batches to avoid memory issues\"\"\"\n",
    "    predictions = []\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch_pred = model.predict(X[start_idx:end_idx])\n",
    "        predictions.extend(batch_pred)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "print(\"Making predictions...\")\n",
    "y_train_pred = predict_in_batches(rf_model, X_train_scaled, batch_size=10000)\n",
    "y_test_pred = predict_in_batches(rf_model, X_test_scaled, batch_size=10000)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n=== RESULTS ===\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\n=== CLASSIFICATION REPORT ===\")\n",
    "if target_encoder:\n",
    "    target_names = target_encoder.classes_\n",
    "    print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
    "else:\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n=== TOP 10 MOST IMPORTANT FEATURES ===\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bfda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save Models Efficiently\n",
    "print(\"\\n=== SAVING MODELS ===\")\n",
    "\n",
    "# Use joblib for large models (more efficient than pickle)\n",
    "print(\"Saving scaler...\")\n",
    "dump(scaler, 'scaler.pkl', compress=3)\n",
    "print(\"Scaler saved as 'scaler.pkl'\")\n",
    "\n",
    "print(\"Saving model...\")\n",
    "dump(rf_model, 'model.pkl', compress=3)\n",
    "print(\"Model saved as 'model.pkl'\")\n",
    "\n",
    "if target_encoder:\n",
    "    dump(target_encoder, 'target_encoder.pkl', compress=3)\n",
    "    print(\"Target encoder saved\")\n",
    "\n",
    "if label_encoders:\n",
    "    dump(label_encoders, 'label_encoders.pkl', compress=3)\n",
    "    print(\"Label encoders saved\")\n",
    "\n",
    "# Save feature names\n",
    "dump(list(X_train.columns), 'feature_names.pkl', compress=3)\n",
    "print(\"Feature names saved\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'n_features': X_train_scaled.shape[1],\n",
    "    'n_train_samples': len(X_train_scaled),\n",
    "    'n_test_samples': len(X_test_scaled),\n",
    "    'sample_fraction_used': SAMPLE_FRACTION,\n",
    "    'model_params': rf_model.get_params()\n",
    "}\n",
    "\n",
    "dump(metadata, 'model_metadata.pkl')\n",
    "print(\"Model metadata saved\")\n",
    "\n",
    "print(f\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\" Model Training Completed Successfully!\")\n",
    "print(f\" Final Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\" Training Samples: {len(X_train_scaled):,}\")\n",
    "print(f\" Test Samples: {len(X_test_scaled):,}\")\n",
    "print(f\" Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\" Peak Memory Usage: {get_memory_usage():.2f} GB\")\n",
    "print(f\" All model files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Memory-Efficient Model Loading Test\n",
    "def test_saved_model():\n",
    "    \"\"\"Test loading and using the saved model\"\"\"\n",
    "    print(\"\\n=== TESTING SAVED MODEL ===\")\n",
    "    \n",
    "    # Load components\n",
    "    loaded_scaler = load('scaler.pkl')\n",
    "    loaded_model = load('model.pkl')\n",
    "    feature_names = load('feature_names.pkl')\n",
    "    metadata = load('model_metadata.pkl')\n",
    "    \n",
    "    print(\" All components loaded successfully!\")\n",
    "    print(f\" Saved model accuracy: {metadata['test_accuracy']:.4f}\")\n",
    "    \n",
    "    # Test with a small sample\n",
    "    if len(X_test_scaled) > 0:\n",
    "        test_sample = X_test_scaled[:5]\n",
    "        predictions = loaded_model.predict(test_sample)\n",
    "        print(f\" Sample predictions: {predictions}\")\n",
    "    \n",
    "    return loaded_model, loaded_scaler\n",
    "\n",
    "# Run the test\n",
    "test_saved_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
